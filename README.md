This is the code for paper Can Large Language Models Serve as Anomaly Detectors on Dynamic Graphs?
# Reference list

- [Can Language Models Solve Graph Problems in Natural Language? (Arxiv '24)](https://arxiv.org/html/2305.10037v3)

<!--## Overview
Graph Foundation Models (GFMs) represent a rapidly advancing area in machine learning, leveraging graph structures and large-scale pre-training for applications across diverse domains. This repository highlights existing GFM research, provides references, and showcases our findings and experiments.

---

## Existing GFM Papers
### Domain-Specific GFMs

Below is a curated list of papers focusing on domain-specific Graph Foundation Models:

- **Protein Modeling**:
  - [Language models of protein sequences at the scale of evolution enable accurate structure prediction (BioRxiv '22)](https://biorxiv.org/)

- **Molecular and Material Modeling**:
  - [A foundation model for atomistic materials chemistry (Arxiv '24)](https://arxiv.org/)
  - [From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction (ICLR '24)](https://openreview.net/)
  - [Towards Predicting Equilibrium Distributions for Molecular Systems with Deep Learning (Arxiv '23)](https://arxiv.org/)
  - [DPA-2: Towards a universal large atomic model for molecular and material simulation (Arxiv '23)](https://arxiv.org/)
  - [MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures (Arxiv '24)](https://arxiv.org/)
  - [Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules (ICLR '23)](https://openreview.net/)
  - [On the Scalability of GNNs for Molecular Graphs (Arxiv '24)](https://arxiv.org/)
  - [MiniMol: A Parameter-Efficient Foundation Model for Molecular Learning (Arxiv '24)](https://arxiv.org/)
  - [A Graph is Worth K Words: Euclideanizing Graph using Pure Transformer (ICML '24)](https://proceedings.mlr.press/)

---

## Repository Structure
The repository contains the following folders:

- **`code/`**: Code for experiments and model implementation.
- **`data/`**: Processed datasets used in the experiments.
- **`models/`**: Pre-trained models and configurations for reproducing our results.
- **`results/`**: Results and analysis from our experiments.

---

## Requirements
To reproduce the results, you will need:

- Python 3.8+
- PyTorch 2.0+
- Additional dependencies listed in `requirements.txt`

Install the dependencies using:

```bash
pip install -r requirements.txt
```

---

## Running the Code

### 1. Data Preparation
Download the datasets and preprocess them:
```bash
python data_preparation.py
```

### 2. Training
Train the GFM model on your chosen dataset:
```bash
python train.py --config configs/config.yaml
```

### 3. Evaluation
Evaluate the trained model:
```bash
python evaluate.py --checkpoint models/checkpoint.pth
```

---

## Citation
If you use this work, please cite:

```bibtex
@inproceedings{yourpaper2024,
  title={Position: Graph Foundation Models Are Already Here},
  author={Your Name and Collaborators},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  year={2024},
}
```

---

## Acknowledgments
We thank the authors of the referenced papers for their contributions to the GFM community.

---

## License
This repository is licensed under the MIT License. See `LICENSE` for details. -->
